{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea0ebe6a-f037-4536-820b-c12b2254b391",
   "metadata": {},
   "source": [
    "## Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aed12e-22ba-4ea7-ac24-d1c4c219a094",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "stream_session= SparkSession.builder.appName(\"streaming\").config(\"spark.executor.memory\", \"4g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21312bca-1737-444d-8527-82fc9b1ee3a8",
   "metadata": {},
   "source": [
    "Here,the spark streaming session is demonstrating the records that were passed via kafka and pushed into csv files as batches.These are then  then  merged together with the master csv file gun-violence.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "505f2a1d-4d8d-4c5b-be5e-3676c989f3d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, TimestampType\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"incident_id\", StringType(), True),\n",
    "    StructField(\"date\", TimestampType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"city_or_county\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"n_killed\", IntegerType(), True),\n",
    "    StructField(\"n_injured\", IntegerType(), True),\n",
    "    StructField(\"incident_url\", StringType(), True),\n",
    "    StructField(\"source_url\", StringType(), True),\n",
    "    StructField(\"incident_url_fields_missing\", BooleanType(), True),\n",
    "    StructField(\"congressional_district\", IntegerType(), True),\n",
    "    StructField(\"gun_stolen\", StringType(), True),\n",
    "    StructField(\"gun_type\", StringType(), True),\n",
    "    StructField(\"incident_characteristics\", StringType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"location_description\", StringType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"n_guns_involved\", IntegerType(), True),\n",
    "    StructField(\"notes\", StringType(), True),\n",
    "    StructField(\"participant_age\", StringType(), True),\n",
    "    StructField(\"participant_age_group\", StringType(), True),\n",
    "    StructField(\"participant_gender\", StringType(), True),\n",
    "    StructField(\"participant_name\", StringType(), True),\n",
    "    StructField(\"participant_relationship\", StringType(), True),\n",
    "    StructField(\"participant_status\", StringType(), True),\n",
    "    StructField(\"participant_type\", StringType(), True),\n",
    "    StructField(\"sources\", StringType(), True),\n",
    "    StructField(\"state_house_district\", IntegerType(), True),\n",
    "    StructField(\"state_senate_district\", IntegerType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50692772-e9ab-48ec-8e37-e363ac67b285",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/11 00:50:13 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/90/k1t6q47n6wq06bm2v8x2fdf40000gn/T/temporary-e7be039c-1bdc-4794-8c89-d72796dd04f4. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/11 00:50:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/03/11 00:50:14 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "incident_stream=stream_session.readStream.schema(schema).csv('newDir/')\n",
    "\n",
    "incquery=incident_stream.writeStream.queryName(\"incidenttable\").format(\"memory\").outputMode(\"append\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3eba21-906c-4713-95d9-1b339afe40f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range (30):\n",
    "    stream_session.sql(\"SELECT count(*) AS TotalIncidents FROM incidenttable\").show()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd577b71-1281-43b4-b4c8-02ac6b54718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All batch files are merged into gun-violence-new.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "# Output directory containing batch files\n",
    "input_directory = 'newDir/'\n",
    "\n",
    "# Output file for the combined data\n",
    "output_file = 'gun-violence.csv'\n",
    "\n",
    "# Initialize a flag to check if the output file header is written\n",
    "header_written = False\n",
    "\n",
    "try:\n",
    "    # Open the output file in append mode\n",
    "    with open(output_file, 'a', newline='') as output_csv:\n",
    "        csv_writer = csv.writer(output_csv)\n",
    "        \n",
    "        # Iterate through batch files\n",
    "        for filename in sorted(os.listdir(input_directory)):\n",
    "            if filename.endswith('.csv'):\n",
    "                batch_file_path = os.path.join(input_directory, filename)\n",
    "                \n",
    "                # Open the batch file\n",
    "                with open(batch_file_path, 'r') as batch_file:\n",
    "                    csv_reader = csv.reader(batch_file)\n",
    "                    \n",
    "                    # Skip the header if it's already written to the output file\n",
    "                    if header_written:\n",
    "                        next(csv_reader)\n",
    "                    else:\n",
    "                        header_written = True\n",
    "                    \n",
    "                    # Iterate through rows in the batch file and append them to the output file\n",
    "                    for row in csv_reader:\n",
    "                        csv_writer.writerow(row)\n",
    "    \n",
    "    print(f\"All batch files are merged into {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
